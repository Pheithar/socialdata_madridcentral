{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c647838f",
   "metadata": {},
   "source": [
    "# MADRID CENTRAL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01853c19",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Different libraries and functions we require to run the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b69e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib.path import Path\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import time\n",
    "from datetime import datetime as dt\n",
    "import utm\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "import zipfile\n",
    "import os\n",
    "import io\n",
    "from collections import defaultdict\n",
    "\n",
    "# folium\n",
    "import folium\n",
    "import folium.plugins as plugings\n",
    "\n",
    "# bokeh\n",
    "import bokeh.palettes\n",
    "from bokeh.plotting import figure, show, output_file, save, reset_output\n",
    "from bokeh.io import output_notebook\n",
    "from bokeh.models import HoverTool, Legend, ColumnDataSource, Span, ColorBar, DatetimeTickFormatter, CustomJS, Button\n",
    "from bokeh.models.widgets import Panel, Tabs\n",
    "from bokeh.tile_providers import get_provider, CARTODBPOSITRON\n",
    "from bokeh.layouts import layout\n",
    "from bokeh.transform import linear_cmap\n",
    "\n",
    "\n",
    "output_notebook()\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4f01f9",
   "metadata": {},
   "source": [
    "## Auxiliar Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9d533a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_color_from_palette(color):\n",
    "    \"\"\" Getting colors for plotting \"\"\"\n",
    "    return tuple([int(c * 255) for c in color])\n",
    "\n",
    "def get_dark_color_from_palette(color):\n",
    "    \"\"\" Getting darker colors for plotting \"\"\"\n",
    "    return tuple([int(c * 200) for c in color])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fa3378",
   "metadata": {},
   "source": [
    "## Constants\n",
    "\n",
    "Different constants to use through the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca87b2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOWNLOAD_DATA = False # Dowload or not the data\n",
    "GENERATE_TRAFFIC_DATA = False # Generate traffic data\n",
    "\n",
    "SAVE_PLOTS = False # If true, save the bokeh plots as html. Othewise, show\n",
    "\n",
    "PALETTE = \"colorblind\"\n",
    "\n",
    "DISTRICT_COLORS = [\n",
    "    get_color_from_palette(c)\n",
    "    for c in sns.color_palette(PALETTE, 21)\n",
    "    ]\n",
    "\n",
    "DISTRICT_DARK_COLORS = [\n",
    "    get_dark_color_from_palette(c)\n",
    "    for c in sns.color_palette(PALETTE, 21)\n",
    "    ]\n",
    "\n",
    "MADRID_IN_OUT_COLORS = [\n",
    "    get_color_from_palette(c)\n",
    "    for c in sns.color_palette(PALETTE, 2)\n",
    "    ]\n",
    "\n",
    "MADRID_IN_OUT_DARK_COLORS = [\n",
    "    get_dark_color_from_palette(c)\n",
    "    for c in sns.color_palette(PALETTE, 2)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29446f3c",
   "metadata": {},
   "source": [
    "## 1. Motivation\n",
    "\n",
    "The purpose of our project is to analyze the real impact of the measure that took place in the city center of Madrid called **Madrid Central**.\n",
    "\n",
    "To sum up, Madrid Central was an environment measure that entered in force the **30th of November 2018**, it prohibited all polluting vehicles from entering the *\"Centro\"* district of Madrid. Unfortunately, it was taken down for political and economic reasons the **1st of July 2019**. \n",
    "\n",
    "In this study, we aim to evaluate the effectiveness of this vehicle regulation on pollution by analyzing traffic and air quality.\n",
    "\n",
    "More on **Madrid Central** on the webpage.\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"https://s03.s3c.es/imag/_v0/864x1252/2/7/c/Madrid-Central_Plano-con-logo.jpg\" alt=\"Madrid Central\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fceef3a1",
   "metadata": {},
   "source": [
    "### The Dataset\n",
    "\n",
    "We use multiple datasets, all of them from the **Open Data platform of the Town hall of Madrid**. \n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"https://www.zupimages.net/up/22/18/htou.png\" alt=\"Open Data Madrid\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "The datasets we use are:\n",
    "\n",
    " - **Air Quality Data**\n",
    "    - [*Calidad del aire. Datos horarios desde 2001* (*Air quality. Hourly data since 2001*)](https://datos.madrid.es/sites/v/index.jsp?vgnextoid=f3c0f7d512273410VgnVCM2000000c205a0aRCRD&vgnextchannel=374512b9ace9f310VgnVCM100000171f5a0aRCRD):\n",
    "    \n",
    "      This data has in each file different air quality measures from multiple measurement stations. There are 24 stations all around the city of Madrid, with different gases measured in each one of them.\n",
    "\n",
    "    - [*Calidad del aire. Estaciones de control* (*Air quality. Control stations*)](https://datos.madrid.es/portal/site/egob/menuitem.c05c1f754a33a9fbe4b2e4b284f1a5a0/?vgnextoid=9e42c176313eb410VgnVCM1000000b205a0aRCRD&vgnextchannel=374512b9ace9f310VgnVCM100000171f5a0aRCRD&vgnextfmt=default):\n",
    "    \n",
    "      This is the geodata related to the measurement points. For plotting using Bokeh we had to change the format of how the spatial coordinates are presented.\n",
    "\n",
    " - **Traffic Data**\n",
    "    - [*Tráfico. Histórico de datos del tráfico desde 2013* (*Traffic. Historic traffic data since 2013*)](https://datos.madrid.es/portal/site/egob/menuitem.c05c1f754a33a9fbe4b2e4b284f1a5a0/?vgnextoid=33cb30c367e78410VgnVCM1000000b205a0aRCRD&vgnextchannel=374512b9ace9f310VgnVCM100000171f5a0aRCRD&vgnextfmt=default):\n",
    "    \n",
    "      This data has in separated files the recorded traffic measurements from more than 3000 points, taken every 15 minutes. This is the core dataset we use for our traffic analysis, but, to reduce the size of the data, we preprocess it to only use the average measure on each point per day.\n",
    "    \n",
    "    - [*Tráfico. Ubicación de los puntos de medida del tráfico* (*Traffic. Location of traffic measurement points*)](https://datos.madrid.es/portal/site/egob/menuitem.c05c1f754a33a9fbe4b2e4b284f1a5a0/?vgnextoid=ee941ce6ba6d3410VgnVCM1000000b205a0aRCRD&vgnextchannel=374512b9ace9f310VgnVCM100000171f5a0aRCRD):\n",
    "    \n",
    "      This is the geodata related to the measurement points. For plotting using Bokeh we had to change the format of how the spatial coordinates are presented.\n",
    "\n",
    " - **Districts Geodata**\n",
    "\n",
    "   - [*Distritos municipales de Madrid* (*Municipal districts of Madrid*)](https://datos.madrid.es/sites/v/index.jsp?vgnextoid=7d6e5eb0d73a7710VgnVCM2000001f4a900aRCRD&vgnextchannel=374512b9ace9f310VgnVCM100000171f5a0aRCRD):\n",
    "   \n",
    "      The data has geodata of the limits of each of the districts of Madrid\n",
    "\n",
    "\n",
    "### The Reason of the Dataset\n",
    "\n",
    "We decided on these two datasets because we wanted to discover if **Madrid Central** achieved the objectives of reducing air pollution in the area, and we also wanted to inspect how effective and how big was the change in the traffic in and out of **Madrid Central**.\n",
    "\n",
    "Additionally, we desired to analyze the relation between air quality and traffic in the city of Madrid.\n",
    "\n",
    "### User Experience\n",
    "\n",
    "While investigating this topic, we saw a lot of articles about **Madrid Central**. Some of them, saying how good and beneficial it was. Others, saying the exact opposite. Sadly, most of the news and articles were quite influenced by political ideas. This is because the measure was imposed by a political party, against the wishes of the opposing political party. Therefore, we feel they were biased, and did not rely on data and science.\n",
    "\n",
    "We aim to fix this, trying to give an answer to how effective the measure really was, based on data, using visualizations and data analysis. We also want to present this in an informative and interactive way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17204637",
   "metadata": {},
   "source": [
    "## 2. Basic Stats\n",
    "\n",
    "### Data Processing\n",
    "\n",
    "The first thing that has to be done in any data related task, is to preprocess and clean the data. In our case, this is very important, as we have as baseline many different files, with a lot of information. We have to reduce the dataset to obtain only the relevant parts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e496586",
   "metadata": {},
   "source": [
    "**Air quality Data Processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52e0f7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64d96fc9",
   "metadata": {},
   "source": [
    "**Traffic Data Processing**\n",
    "\n",
    "The data we want to work with is very large, thus we need to download it from the source as it is not possible to upload it to the version control system we use (GitHub). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f76d4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_data_traffic():\n",
    "    \"\"\" Download all traffic data from January 2016 (ID=32) until February 2020 (ID=81)\n",
    "        Some files do not follow the same naming convention, and need repairing.\n",
    "        The name convention that most files follow is '{num_month}-{num_year}.yaml',\n",
    "        so everyone will follow that\n",
    "    \"\"\"\n",
    "    FIRST_MONTH_ID = 32\n",
    "    LAST_MONTH_ID = 81\n",
    "    DATA_PATH = \"data/traffic2\"\n",
    "    \n",
    "    for month_id in tqdm(range(FIRST_MONTH_ID, LAST_MONTH_ID+1), desc=\"Downloading data\", unit=\"file\"):\n",
    "        \n",
    "        # Get month number, from 1 to 12\n",
    "        current_month = ((month_id - FIRST_MONTH_ID) % 12) + 1\n",
    "        \n",
    "        # Get year number, from 2016 to 2021\n",
    "        current_year = int((month_id - FIRST_MONTH_ID) / 12) + 2016    \n",
    "               \n",
    "        # If it has been downloaded already, skip it\n",
    "\n",
    "        file_path = f\"{DATA_PATH}/{current_month:02d}-{current_year}.csv\"\n",
    "\n",
    "        if not os.path.isfile(file_path):\n",
    "\n",
    "            url = f\"https://datos.madrid.es/egob/catalogo/208627-{month_id}-transporte-ptomedida-historico.zip\"\n",
    "            r = requests.get(url)\n",
    "            z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "            zipcsv = z.infolist()[-1]\n",
    "            \n",
    "            # Rename file\n",
    "            zipcsv.filename = file_path\n",
    "            \n",
    "            # Extract file\n",
    "            z.extract(zipcsv)\n",
    "            \n",
    "\n",
    "if DOWNLOAD_DATA:\n",
    "    download_data_traffic()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd11d25",
   "metadata": {},
   "source": [
    "Before diving into the actual data, we need to contextualize. Madrid is divided into districts. There are *21* one of them, being the area of **Madrid Central** exactly the same as the **Centro district** area (thus the name).\n",
    "\n",
    "We have a dataset of where the measure of traffic points are located. As expected, they are not evenly distributed. Our first task is to see in which district each traffic measurement point is located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f610d691",
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic_points = pd.read_csv(\"shared_data/traffic_points/pmed_trafico_03052016.csv\", sep=\";\")\n",
    "traffic_points.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14c87ee",
   "metadata": {},
   "source": [
    "First we need to calculate the correct *utm* for displaying in `bokeh` maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28358d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def utm_from_latlon(lat, lon):\n",
    "    \"\"\" From a given lat and lon, calculates the correct UTM coordinates to \n",
    "        plot using `bokeh` \n",
    "    \"\"\"\n",
    "    r_major = 6378137.000\n",
    "    x = r_major * np.radians(lon)\n",
    "    scale = x/lon\n",
    "    y = 180.0/np.pi * np.log(np.tan(np.pi/4.0 + \n",
    "        lat * (np.pi/180.0)/2.0)) * scale\n",
    "\n",
    "    return x, y\n",
    "\n",
    "def get_lat_lon_utm(row):\n",
    "    \"\"\" From a row containing the columns 'st_x' and 'st_y' calculates both the lat and lon\n",
    "        and the correct UTM coordinates to plot using `bokeh`\n",
    "    \"\"\"\n",
    "\n",
    "    # 30 and 'T' is the zone of Madrid\n",
    "    lat, lon = utm.to_latlon(row[\"st_x\"], row[\"st_y\"], 30, \"T\")\n",
    "    \n",
    "    x, y = utm_from_latlon(lat, lon)\n",
    "\n",
    "    return pd.Series([lat, lon, x, y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1142aab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic_points[[\"latitude\", \"longitude\", \"utm_x\", \"utm_y\"]] = traffic_points.apply(get_lat_lon_utm, axis=1)\n",
    "traffic_points = traffic_points.rename(columns = {'nombre':'name'})\n",
    "traffic_points.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c7a0c3",
   "metadata": {},
   "source": [
    "Then load the districts information to display them in the map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c378df",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"shared_data/districts/districts.geojson\", \"r\") as geojson:\n",
    "    geodata = json.load(geojson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6616827f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_districts = pd.DataFrame([], columns=[\"name\", \"latitude\",\n",
    "                                         \"longitude\", \"utm_x\",\n",
    "                                         \"utm_y\"])\n",
    "for district in geodata[\"features\"]:\n",
    "    # Get district name\n",
    "    district_name = district[\"properties\"][\"NOMBRE\"]\n",
    "    \n",
    "    # Get district coordinates\n",
    "    district_coord = district[\"geometry\"][\"coordinates\"][0]\n",
    "    df_district = pd.DataFrame(district[\"geometry\"][\"coordinates\"][0], columns=[\"st_x\", \"st_y\"])\n",
    "    df_district[\"name\"] = district_name\n",
    "    \n",
    "    # Calculate correct utm\n",
    "    df_district[[\"latitude\", \"longitude\", \"utm_x\", \"utm_y\"]] = df_district.apply(get_lat_lon_utm, axis=1)\n",
    "    df_district = df_district.drop(columns=[\"st_x\", \"st_y\"])\n",
    "    \n",
    "    # Append to all districts dataframe\n",
    "    df_districts = pd.concat([df_districts, df_district]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "district_name = df_districts[\"name\"].unique()\n",
    "df_districts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dda9433",
   "metadata": {},
   "source": [
    "Save in which district is each traffic point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110bf0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic_points[\"district\"] = \"None\"\n",
    "points = traffic_points[[\"utm_x\", \"utm_y\"]]\n",
    "\n",
    "for name in district_name:\n",
    "    path = Path(df_districts[df_districts[\"name\"] == name][[\"utm_x\", \"utm_y\"]])\n",
    "    points_in_path_mask = path.contains_points(points)\n",
    "    traffic_points.loc[points_in_path_mask, \"district\"] = name\n",
    "\n",
    "# Discard the traffic points outside any district of Madrid, as they are outside the city\n",
    "    \n",
    "traffic_points = traffic_points.drop(traffic_points[traffic_points[\"district\"] == \"None\"].index)\\\n",
    "                .reset_index(drop=True)\n",
    "\n",
    "traffic_points.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf673bd0",
   "metadata": {},
   "source": [
    "The next step is to load the datasets for traffic information. This datasets have a lot of rows, as each of the more than 3000 measurement points record mutiple parameters each 15 minutes, so a rough approximation of how many rows each month file has is:\n",
    "\n",
    "$$ 30(days) \\cdot 24(hours) \\cdot 4(measures\\_per\\_hour) \\cdot 3000(traffic\\_points) = 8640000 $$\n",
    "\n",
    "And once again, if we take into account that we are using data from 2016 until the end of 2021, a more accurate row count would be:\n",
    "\n",
    "$$ 4(years) \\cdot 365(days) \\cdot 24(hours) \\cdot 4(measures\\_per\\_hour) \\cdot 3000(traffic\\_points) = 42048000 $$\n",
    "\n",
    "This amount of data (more than 630 million rows) is too much to handle efficiently, and obtain relevant information. To reduce the amount of rows, we decide on keeping the average intensity of traffic (Number of cars) per day in each district. That way, we will have:\n",
    "\n",
    "$$ 4(years) \\cdot 365(days) \\cdot 21(number\\_districts) = 30660 $$\n",
    "\n",
    "which is more manageable number, from where we aspire to detect the relevant information in the data. Around 13714 times less data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b4bb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_traffic_data(filepath, traffic_points_df):\n",
    "    \"\"\" Function to process each traffic data file. This preoprocess has as objective to reduce\n",
    "        the dimensionality od the data, only keeping one value per district per day, reducing this\n",
    "        way the number of rows to handle.\n",
    "        \n",
    "        Arguments:\n",
    "            filepath          -> path to load the csv\n",
    "            traffic_points_df -> traffic_points dataset (where they are located)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load file\n",
    "    traffic_df = pd.read_csv(filepath, sep=\";\")\n",
    "    \n",
    "    # For god knows why, there is one file that is separated by ',' instead of ';'\n",
    "    # so we reread the file if it only has one column\n",
    "    if len(traffic_df.columns) == 1:\n",
    "        traffic_df = pd.read_csv(filepath, sep=\",\")\n",
    "    \n",
    "    # If the 'idelem' column does not exists, is because is called 'id', so rename column\n",
    "    if \"idelem\" not in traffic_df.columns:\n",
    "        traffic_df = traffic_df.rename(columns = {'id':'idelem'})\n",
    "    \n",
    "    # Use only the traffic points for whom we have information \n",
    "    traffic_df = traffic_df[traffic_df[\"idelem\"].isin(traffic_points_df[\"idelem\"])]\n",
    "    \n",
    "    # Transform date to datime type\n",
    "    traffic_df[\"fecha\"] = pd.to_datetime(traffic_df[\"fecha\"])\n",
    "    \n",
    "    # Get date in separate columns\n",
    "    traffic_df[\"day\"] = traffic_df[\"fecha\"].dt.day\n",
    "    traffic_df[\"month\"] = traffic_df[\"fecha\"].dt.month\n",
    "    traffic_df[\"year\"] = traffic_df[\"fecha\"].dt.year\n",
    "\n",
    "    # Group by id and date, up to day, and get the average intensity perr traffic point\n",
    "    traffic_df = traffic_df.groupby([\"idelem\",\n",
    "                                     \"day\",\n",
    "                                     \"month\",\n",
    "                                     \"year\"]).agg(mean_intensity=(\"intensidad\", \"mean\")).reset_index()\n",
    "    \n",
    "    # Merge with the traffic points to get the district for each point\n",
    "    traffic_df = traffic_df.merge(traffic_points_df[[\"idelem\", \"district\"]], on=\"idelem\")\n",
    "    \n",
    "    # Save also the traffic points as separated files\n",
    "    traffic_points_intensity_df = traffic_df.copy()\n",
    "\n",
    "    # Group by again, to get only one value per district per day\n",
    "    traffic_df = traffic_df.groupby([\"district\", \"day\", \"month\", \"year\"]).mean()[\"mean_intensity\"].reset_index()\n",
    "    \n",
    "    # Get the date and day of the week for plotting purpose\n",
    "    traffic_df[\"date\"] = pd.to_datetime(traffic_df[[\"day\", \"month\", \"year\"]])\n",
    "    traffic_df[\"day_of_week\"] = traffic_df[\"date\"].dt.day_name()\n",
    "\n",
    "    traffic_points_intensity_df[\"date\"] = pd.to_datetime(traffic_points_intensity_df[[\"day\", \"month\", \"year\"]])\n",
    "    traffic_points_intensity_df[\"day_of_week\"] = traffic_points_intensity_df[\"date\"].dt.day_name()\n",
    "\n",
    "    # Save idelem as integer\n",
    "    traffic_points_intensity_df[\"idelem\"] = traffic_points_intensity_df[\"idelem\"].astype(int)\n",
    "    \n",
    "    return traffic_df, traffic_points_intensity_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fc20e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_trafic_data(traffic_points_df):\n",
    "    \"\"\" Function to load all trafic data from the data folder,\n",
    "        after being processed\n",
    "        \n",
    "        Arguments:\n",
    "            traffic_points_df -> traffic_points dataset (where they are located)\n",
    "    \"\"\"\n",
    "    \n",
    "    DATA_PATH = \"data/traffic\"\n",
    "    \n",
    "    traffic_data = pd.DataFrame([], columns=[\"district\", \"date\", \"day_of_week\",\n",
    "                                             \"day\", \"month\", \"year\", \"mean_intensity\"])\n",
    "    \n",
    "    traffic_points_data = pd.DataFrame([], columns=[\"district\", \"date\", \"day_of_week\",\n",
    "                                             \"day\", \"month\", \"year\", \"mean_intensity\"])\n",
    "    \n",
    "    for filepath in tqdm(os.listdir(DATA_PATH), desc=\"Processing files\", unit=\"file\"):\n",
    "        traffic_df, traffic_points_intensity_df= process_traffic_data(os.path.join(DATA_PATH, filepath), traffic_points_df)\n",
    "        \n",
    "        traffic_data = pd.concat([traffic_data, traffic_df])\n",
    "        traffic_points_data = pd.concat([traffic_points_data, traffic_points_intensity_df])\n",
    "\n",
    "\n",
    "    return (traffic_data.sort_values(by=[\"district\", \"date\"]).reset_index(drop=\"True\"),\n",
    "            traffic_points_data.sort_values(by=[\"district\", \"date\", \"idelem\"]).reset_index(drop=\"True\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e897ead4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_traffic_path = \"shared_data/traffic_intensity.csv\"\n",
    "df_traffic_intensity_path = \"data/traffic_points_intensity.csv\"\n",
    "\n",
    "\n",
    "if not GENERATE_TRAFFIC_DATA:\n",
    "    total_traffic_df = pd.read_csv(df_traffic_path)\n",
    "    total_traffic_df[\"date\"] = pd.to_datetime(total_traffic_df[\"date\"])\n",
    "\n",
    "    traffic_points_intensity_df = pd.read_csv(df_traffic_intensity_path)\n",
    "    traffic_points_intensity_df[\"date\"] = pd.to_datetime(traffic_points_intensity_df[\"date\"])\n",
    "    traffic_points_intensity_df[\"idelem\"] = traffic_points_intensity_df[\"idelem\"].astype(int)\n",
    "else:\n",
    "    total_traffic_df, traffic_points_intensity_df = load_all_trafic_data(traffic_points)\n",
    "\n",
    "    total_traffic_df.to_csv(df_traffic_path, index=False)\n",
    "    traffic_points_intensity_df.to_csv(df_traffic_intensity_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d494af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_traffic_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497a1826",
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic_points_intensity_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c083479c",
   "metadata": {},
   "source": [
    "### Exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236321cc",
   "metadata": {},
   "source": [
    "**Air quality data exploration**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63e7b71",
   "metadata": {},
   "source": [
    "**Traffic data exploration**\n",
    "\n",
    "We want to check first how many traffic points are in each district."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfd6d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic_points.groupby(\"district\").agg(number_traffic_points=(\"idelem\", \"count\")).sort_values(\"number_traffic_points\").reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38171be3",
   "metadata": {},
   "source": [
    "From the table we can see that there is quite a big difference between the different traffic points, being the district with the least number *Barajas* (41) and the district with the biggest number *Chamartín* (357).\n",
    "\n",
    "This is something we have to take into account, as too few datapoints may result in poor conclusions. Luckly, the *Centro* district has **176** measurement points, which we believe it's enough given the small area (5.23 km²) of this district."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9545edd",
   "metadata": {},
   "source": [
    "Now let's compare the traffic intensity in each district, to see which districts are busier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b096d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_traffic_points = traffic_points.copy()\n",
    "df_traffic_points = pd.merge(df_traffic_points,\n",
    "                             traffic_points_intensity_df.groupby(\"idelem\").agg(intensity=(\"mean_intensity\", \"mean\")).reset_index(),\n",
    "                             on=\"idelem\")\n",
    "\n",
    "df_traffic_points.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f661ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = figure(title=\"Average Intensity per District\", x_axis_type=\"mercator\", y_axis_type=\"mercator\",\n",
    "           height=600, width=600, tools=\"\")\n",
    "\n",
    "p.axis.visible = False\n",
    "p.toolbar.logo = None\n",
    "p.toolbar_location = None\n",
    "\n",
    "\n",
    "for name in district_name:\n",
    "    # Districts\n",
    "    source_dict = dict(utm_x = [[x for x in df_districts[df_districts[\"name\"] == name][\"utm_x\"]]],\n",
    "                       utm_y = [[y for y in df_districts[df_districts[\"name\"] == name][\"utm_y\"]]],\n",
    "                       name = [name],\n",
    "                       intensity = [total_traffic_df[total_traffic_df[\"district\"] == name][\"mean_intensity\"].mean()])\n",
    "\n",
    "    source = ColumnDataSource(source_dict)\n",
    "    p.patches(xs=\"utm_x\", ys=\"utm_y\", color=linear_cmap(\"intensity\", \"Viridis256\", 100, 700), line_width=3, alpha=0.4, \n",
    "            source=source, muted=False, muted_alpha=0.1)\n",
    "\n",
    "    \n",
    "\n",
    "source = ColumnDataSource(df_traffic_points)\n",
    "\n",
    "p.circle(x=\"utm_x\", y=\"utm_y\", line_width=1, color=linear_cmap(\"intensity\", \"Viridis256\", 100, 700),\n",
    "    source=source, alpha=0.75, size=3)\n",
    "    \n",
    "\n",
    "cartodb = get_provider(CARTODBPOSITRON)\n",
    "p.add_tile(cartodb)\n",
    "\n",
    "TOOLTIPS = [\n",
    "    (\"District Name\", \"@name\"),\n",
    "    (\"Average Intensity\", \"@intensity\"),\n",
    "]\n",
    "p.add_tools(HoverTool(tooltips=TOOLTIPS))\n",
    "\n",
    "p.background_fill_color = None\n",
    "p.border_fill_color = None\n",
    "\n",
    "mapper = linear_cmap(field_name='intensity', palette=\"Viridis256\", low=100, high=700)\n",
    "\n",
    "color_bar = ColorBar(color_mapper=mapper['transform'], width=8)\n",
    "\n",
    "p.add_layout(color_bar, 'right')\n",
    "\n",
    "p = layout(p, sizing_mode='scale_both')\n",
    "\n",
    "if SAVE_PLOTS:\n",
    "    output_file(\"html_plots/average_intensity_districts.html\", title=\"Average Intensity Districts Map\")\n",
    "    save(p)\n",
    "\n",
    "    reset_output()\n",
    "\n",
    "    output_notebook()\n",
    "else:\n",
    "    show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd420d9",
   "metadata": {},
   "source": [
    "From this plot, we can see that **Centro** is not the most transited. We do not know yet if it is thanks to the measures, or that it is just not a very transited district. What we do now for sure, and that can be also powered by the measures, the surrounded districts, such as **Arganzuela**, **Retiro** or **Moncloa** are some of the busiest districts in the city, so it will be interesting to take that into account and try to detect if there is a border effect because of **Madrid Central**.  \n",
    "\n",
    "If we focus more on the traffic points as separated measures instead of districts as a whole, we can observe an expected result. The traffic intensity in the main roads of Madrid are busier than the secondary roads. We can see that, inside **Madrid Central**, the most crowded road is ***Gran Via***, which is the main commercial road in the district. We also appreciate a lot of traffic sorrounding **Madrid Central**, which again may be interesting to investigate if is a normal traffic flow, or is because of the measures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51de1d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = figure(title=\"Traffic intensity through time by district\", x_axis_label=\"Date\",\n",
    "           y_axis_label=\"Traffic intensity\")\n",
    "\n",
    "fig_lines = []\n",
    "\n",
    "for name, color in zip(district_name, DISTRICT_COLORS):\n",
    "    source = ColumnDataSource(total_traffic_df[total_traffic_df[\"district\"] == name])\n",
    "    l = p.line(x=\"date\", y=\"mean_intensity\", source=source,\n",
    "               color=color, legend_label=name, visible=True,\n",
    "               line_width=3, alpha=0.8)\n",
    "    fig_lines.append(l)\n",
    "    \n",
    "p.renderers.extend(fig_lines)\n",
    "\n",
    "    \n",
    "p.add_layout(p.legend[0], \"right\")\n",
    "p.legend.click_policy = \"hide\"\n",
    "\n",
    "p.xaxis.formatter=DatetimeTickFormatter(\n",
    "        days=['%a %d/%m/%Y'],\n",
    "        months=['%b %Y'],\n",
    "        years = ['%Y']\n",
    "    )\n",
    "\n",
    "# Hover tooltip\n",
    "TOOLTIPS = [\n",
    "    (\"District\", \"@district\"),\n",
    "    (\"Intensity\", \"@mean_intensity\"),\n",
    "    (\"Day\", \"@day_of_week @day/@month/@year\")\n",
    "]\n",
    "p.add_tools(HoverTool(tooltips=TOOLTIPS, mode=\"vline\"))\n",
    "\n",
    "# Button\n",
    "button = Button(\n",
    "    label=\"Switch all lines visibility\", button_type=\"success\", max_width=100, max_height=50\n",
    ")\n",
    "callback = CustomJS(args=dict(lines=fig_lines),\n",
    "    code=\"\"\"\n",
    "    for(var i=0; i<lines.length; i++){\n",
    "        lines[i].visible = !lines[i].visible;\n",
    "    }\n",
    "    \"\"\"\n",
    ")\n",
    "button.js_on_click(callback)\n",
    "\n",
    "\n",
    "p = layout(p, button, sizing_mode=\"scale_both\")\n",
    "\n",
    "\n",
    "if SAVE_PLOTS:\n",
    "    output_file(\"html_plots/traffict_intensity_time_district.html\", title=\"Traffic intensity through time by district\")\n",
    "    save(p)\n",
    "\n",
    "    reset_output()\n",
    "\n",
    "    output_notebook()\n",
    "else:\n",
    "    show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d14f5d",
   "metadata": {},
   "source": [
    "## 3. Data analysis\n",
    "\n",
    "Once we have done our exploratory analysis, it is important to obtain conclusions doing a more in depth analysis of the relevant information obtained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2944127c",
   "metadata": {},
   "source": [
    "**Air Quality Data Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d051d6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f0874849",
   "metadata": {},
   "source": [
    "**Traffic Data Analysis**\n",
    "\n",
    "In our exploratory analysis we visualized a static map. But to see if there is a difference we have to analyze if there is a significant difference between before the regulations and after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f85dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5b81ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_traffic_points = traffic_points.copy()\n",
    "df_traffic_points = pd.merge(df_traffic_points,\n",
    "                             traffic_points_intensity_df.groupby([\"idelem\", \"year\", \"month\"]).agg(intensity=(\"mean_intensity\", \"mean\")).reset_index(),\n",
    "                             on=\"idelem\")\n",
    "\n",
    "df_traffic_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2670c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "map = folium.Map(location=[40.420177, -3.703928], zoom_start=12, tiles='cartodb positron')\n",
    "\n",
    "\n",
    "heatmap_time_data = defaultdict(list)\n",
    "\n",
    "for _, row in (df_traffic_points.iterrows()):\n",
    "    key = str(row[\"month\"]) + \" \" + str(row[\"year\"])\n",
    "    heatmap_time_data[key].append([row[\"latitude\"], row[\"longitude\"]])\n",
    "\n",
    "\n",
    "\n",
    "folium.plugins.HeatMapWithTime(data=list(heatmap_time_data.values()), index=list(heatmap_time_data.keys()) , radius=5, auto_play=True).add_to(map)\n",
    "map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80da9f97",
   "metadata": {},
   "source": [
    "Ok, this map as informative as we thought it would be. We can see small changes in different districts, but nothing too significative. That is why we will try to carry out a different visualization, in an attempt to display the changes more clearly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c4cc79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f9929c7f",
   "metadata": {},
   "source": [
    "## 4. Genre\n",
    "\n",
    "### Genre of the data story\n",
    "\n",
    "We are going to focus on a **Partitioned Poster**, with a special focus on use user interaction. Why? because we believe ...\n",
    "\n",
    "REVISIT THIS ALL TOGETHER TO DECIDE AND WRITE MORE\n",
    "\n",
    "### Visual Narrative\n",
    "\n",
    "**Visual Structuring**: Consistent Visual Platform\n",
    "\n",
    "**Highlighting**: Close-Ups\n",
    "\n",
    "**Transition Guidance**: Object Continuity\n",
    "\n",
    "### Narrative Structure\n",
    "\n",
    "**Ordering**: User Directed Path\n",
    "\n",
    "**Interractivity**: Hover Highlighting / Details and Filtering / Selection / Search\n",
    "\n",
    "**Messaging**: Introductory Text, Captions/Headlines and Summary / Synthesis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c594f37b",
   "metadata": {},
   "source": [
    "## 5. Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a319fdf6",
   "metadata": {},
   "source": [
    "## 6. Discussions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86552d94",
   "metadata": {},
   "source": [
    "## 7. Contributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad4ca74",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09677855",
   "metadata": {},
   "source": [
    "# Constants and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5b54ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def utm_from_latlon(lat, lon):\n",
    "    \"\"\" From a given lat and lon, calculates the correct UTM coordinates to \n",
    "        plot using `bokeh` \n",
    "    \"\"\"\n",
    "    r_major = 6378137.000\n",
    "    x = r_major * np.radians(lon)\n",
    "    scale = x/lon\n",
    "    y = 180.0/np.pi * np.log(np.tan(np.pi/4.0 + \n",
    "        lat * (np.pi/180.0)/2.0)) * scale\n",
    "\n",
    "    return x, y\n",
    "\n",
    "def get_lat_lon_utm(row):\n",
    "    \"\"\" From a row containing the columns 'st_x' and 'st_y' calculates both the lat and lon\n",
    "        and the correct UTM coordinates to plot using `bokeh`\n",
    "    \"\"\"\n",
    "\n",
    "    # 30 and 'T' is the zone of Madrid\n",
    "    lat, lon = utm.to_latlon(row[\"st_x\"], row[\"st_y\"], 30, \"T\")\n",
    "    \n",
    "    x, y = utm_from_latlon(lat, lon)\n",
    "\n",
    "    return pd.Series([lat, lon, x, y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affaccd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_color_from_palette(color):\n",
    "    \"\"\" Getting colors for plotting \"\"\"\n",
    "    return tuple([int(c * 255) for c in color])\n",
    "\n",
    "def get_dark_color_from_palette(color):\n",
    "    \"\"\" Getting darker colors for plotting \"\"\"\n",
    "    return tuple([int(c * 200) for c in color])\n",
    "\n",
    "PALETTE = \"colorblind\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e94f774",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_color_from_palette(sns.color_palette(PALETTE)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbed616",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8597e4b8",
   "metadata": {},
   "source": [
    "# Madrid Central\n",
    "\n",
    "We are going to analyze the impact of Madrid Central both from an air quality and a traffic viewpoint.\n",
    "\n",
    "## Air Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee95c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load air quality stations\n",
    "df_stations = pd.read_csv('shared_data/air_quality/air_quality_stations.csv')\n",
    "\n",
    "# load magnitud table\n",
    "df_magnitud = pd.read_csv('shared_data/air_quality/air_quality_magnitud.csv', sep=';')\n",
    "\n",
    "# load air quality data\n",
    "df = pd.read_csv('data/air_quality_data.csv')\n",
    "\n",
    "# converting Date to datetime type\n",
    "df[\"datetime\"] = pd.to_datetime(df[\"datetime\"])\n",
    "\n",
    "# merge with air quality stations\n",
    "df = pd.merge(df, df_stations, left_on = 'PUNTO_MUESTREO', right_on='punto_muestreo', how='left').drop('PUNTO_MUESTREO', axis=1)\n",
    "\n",
    "# merge with air quality magnitud\n",
    "df = pd.merge(df, df_magnitud, left_on = 'MAGNITUD', right_on='magnitud_id', how='left').drop('MAGNITUD', axis=1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c910d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('AIR QUALITY STATION NAMES:')\n",
    "print([name for name in df.name.unique()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2282752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAP\n",
    "# load MC area\n",
    "cm_points = pd.read_csv('shared_data/districts/central_madrid_points.csv')\n",
    "\n",
    "points = df_stations[[\"utm_x\", \"utm_y\"]].values\n",
    "path = Path(cm_points[[\"utm_x\", \"utm_y\"]].values)\n",
    "points_in_path_mask = path.contains_points(points)\n",
    "\n",
    "df_stations[\"madrid_central\"] = False\n",
    "\n",
    "df_stations.loc[points_in_path_mask, \"madrid_central\"] = True\n",
    "\n",
    "df_stations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2282752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAP\n",
    "\n",
    "# load MC area\n",
    "cm_points = pd.read_csv('shared_data/districts/central_madrid_points.csv')\n",
    "\n",
    "points = df_stations[[\"utm_x\", \"utm_y\"]].values\n",
    "path = Path(cm_points[[\"utm_x\", \"utm_y\"]].values)\n",
    "points_in_path_mask = path.contains_points(points)\n",
    "\n",
    "# plot map\n",
    "p = figure(title=\"Air quality stations in Madrid\", x_axis_type=\"mercator\", y_axis_type=\"mercator\")\n",
    "\n",
    "source = ColumnDataSource(df_stations)\n",
    "cr = p.circle(x=\"utm_x\", y=\"utm_y\",  size=10, source=source)\n",
    "\n",
    "cartodb = get_provider(CARTODBPOSITRON)\n",
    "p.add_tile(cartodb)\n",
    "\n",
    "p.add_tools(HoverTool(tooltips=[('Name', '@name')], renderers=[cr_in, cr_out]))\n",
    "\n",
    "# add interactive legend\n",
    "# legend = Legend(items=[(, [cr_in]), ('OUT of Madrid Central area', [cr_out])], location='center') \n",
    "p.legend.click_policy=\"hide\"\n",
    "# legend.location = \"top_left\"\n",
    "\n",
    "p = layout(p, sizing_mode='scale_both')\n",
    "\n",
    "# output_file(\"html_plots/air_quality_stations.html\", title=\"Air quality stations\")\n",
    "# save(p)\n",
    "# reset_output()\n",
    "# output_notebook()\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5bf8db",
   "metadata": {},
   "source": [
    "Air quality in different stations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a315abe7",
   "metadata": {},
   "source": [
    "Air quality difference respect previous years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e4b563",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_month_year(aRow):\n",
    "    return aRow.name.month_name() + ' ' + str(aRow.name.year)\n",
    "\n",
    "def get_stats_dataframe(station):\n",
    "    df1 = df[(df.name == station)][['formula','value','datetime']]\n",
    "    df1 = df1.pivot(index='datetime', columns='formula', values='value').reset_index()\n",
    "    df1['datetime'] = df1.datetime.dt.floor('D')\n",
    "    df1['datetime'] = df1['datetime'].apply(lambda dt: dt.replace(day=1))\n",
    "    tracking_gas = df1.columns.values[1:]\n",
    "    # cut outliers!!!\n",
    "    if station == 'Plaza del Carmen':\n",
    "        df1 = df1[df1.SO2 <500 ]\n",
    "        df1 = df1[df1.CO < 10 ]\n",
    "    # homogenization (everything in µg/m^3)\n",
    "    for aGas in df1.columns.values[1:]:\n",
    "        unit = df_magnitud[df_magnitud.formula==aGas].unit_per_m3.values[0]\n",
    "        if ((unit == 'mg') or (unit == '10μg')):\n",
    "            # we change to 10\n",
    "            df1[aGas] = df1[aGas] * 100 #*1000 to get it in μg/m3 exactly\n",
    "            idx = df_magnitud[df_magnitud.formula==aGas].index\n",
    "            df_magnitud.loc[idx, 'unit_per_m3'] = '10μg'\n",
    "    # get all stats\n",
    "    df1_stats = df1.groupby(['datetime']).agg(['mean','std'])\n",
    "    df1_stats.columns = df1_stats.columns.to_flat_index()\n",
    "    df1_stats.columns = pd.Index([a+'_'+b for a,b in df1_stats.columns])\n",
    "    df1_stats['date'] = df1_stats.apply(get_month_year, axis=1)\n",
    "    for aGas in tracking_gas:\n",
    "        meanColumn = aGas+'_mean'\n",
    "        stdColumn = aGas+'_std'\n",
    "        df1_stats[aGas+'_upper'] = df1_stats[meanColumn]+df1_stats[aGas+'_std']\n",
    "        df1_stats[aGas+'_lower'] = df1_stats[meanColumn]-df1_stats[aGas+'_std']\n",
    "    return tracking_gas, df1_stats\n",
    "\n",
    "def get_bokeh_viz_evolution_over_time(df1_stats, aText, tracking_gas):\n",
    "\n",
    "    # create annotations for time marks\n",
    "    startMC = time.mktime(dt(2018, 11, 30, 0, 0, 0).timetuple())*1000\n",
    "    startMC_span = Span(location=startMC,\n",
    "                                dimension='height', line_color='black',\n",
    "                                line_dash='dashed', line_width=2, line_alpha=0.3)\n",
    "\n",
    "    finesMC = time.mktime(dt(2019, 3, 15, 0, 0, 0).timetuple())*1000\n",
    "    finesMC_span = Span(location=finesMC,\n",
    "                                dimension='height', line_color='black',\n",
    "                                line_dash='dashed', line_width=2, line_alpha=0.3)\n",
    "\n",
    "    endMC = time.mktime(dt(2019, 7, 1, 0, 0, 0).timetuple())*1000\n",
    "    endMC_span = Span(location=endMC,\n",
    "                                dimension='height', line_color='black',\n",
    "                                line_dash='dashed', line_width=2, line_alpha=0.3)\n",
    "\n",
    "    cds_stats = ColumnDataSource(data=df1_stats)\n",
    "\n",
    "    p = figure(\n",
    "        x_axis_type=\"datetime\",\n",
    "        width=950,\n",
    "        height=450,\n",
    "        title='Evolution of pollutant concentrations over time in '+aText, \n",
    "        y_axis_label='Gas Concentration', \n",
    "        x_axis_label='Date'\n",
    "    )\n",
    "\n",
    "    # create color palette\n",
    "    colors_gas = dict(zip(tracking_gas,list(bokeh.palettes.brewer['Dark2'][len(tracking_gas)])))\n",
    "\n",
    "    # add the data of each gas + interactive legend\n",
    "    lines, circles, bands = {}, {}, {}\n",
    "    items = [] \n",
    "    for aGas in tracking_gas:\n",
    "        unit = df_magnitud[df_magnitud.formula==aGas].unit_per_m3.values[0]\n",
    "        # add line of mean\n",
    "        lines[aGas] = p.line('datetime', aGas+'_mean', source=cds_stats, color = colors_gas[aGas])\n",
    "        # add dots of mean\n",
    "        circles[aGas] = p.circle('datetime',aGas+'_mean', source=cds_stats, color=colors_gas[aGas], size=5, alpha=0.5)\n",
    "        p.add_tools(HoverTool(tooltips=[\n",
    "            ('Gas',aGas),\n",
    "            ('Date', '@date'),\n",
    "            ('Average value', f'@{aGas}_mean {unit}/m3'), \n",
    "            ('Standard Deviation', f'@{aGas}_std {unit}/m3')\n",
    "        ], renderers=[circles[aGas]]))\n",
    "        # add variance\n",
    "        bands[aGas] = p.varea(x='datetime', y1=aGas+'_upper', y2=aGas+'_lower', source=cds_stats, fill_alpha=0.1, fill_color=colors_gas[aGas])\n",
    "        # append legend list\n",
    "        items.append((f'{aGas} ({unit}/m3)', [lines[aGas], circles[aGas], bands[aGas]]))\n",
    "\n",
    "    # add legend\n",
    "    legend = Legend(items=items, location='center') \n",
    "    legend.click_policy=\"hide\"\n",
    "    legend.location = 'top_left'\n",
    "    p.add_layout(legend)\n",
    "\n",
    "    # add annotations to plot\n",
    "    p.add_layout(startMC_span)\n",
    "    p.add_layout(finesMC_span)\n",
    "    p.add_layout(endMC_span)\n",
    "    \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711bed60",
   "metadata": {},
   "outputs": [],
   "source": [
    "stations = ['Plaza del Carmen', \"Plaza de España\", \"Castellana\", \"Retiro\", \"Méndez Álvaro\"]\n",
    "\n",
    "tabs = []\n",
    "\n",
    "for station in stations:\n",
    "    all_tracking_gas, df_stats_station = get_stats_dataframe(station)\n",
    "    p = get_bokeh_viz_evolution_over_time(df_stats_station, station, all_tracking_gas)\n",
    "\n",
    "    p = layout(p, sizing_mode='stretch_both')\n",
    "\n",
    "    tabs.append(Panel(child=p, title=station))\n",
    "\n",
    "tabs = Tabs(tabs=tabs)\n",
    "\n",
    "# output_file(\"html_plots/air_quality_evolution_tabs.html\")\n",
    "# save(tabs)\n",
    "# reset_output()\n",
    "# output_notebook()\n",
    "\n",
    "show(tabs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94dab0d0",
   "metadata": {},
   "source": [
    "## Traffic points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab01c022",
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic_points = pd.read_csv(\"shared_data/traffic_points/pmed_trafico_03052016.csv\", sep=\";\")\n",
    "\n",
    "traffic_points[[\"latitude\", \"longitude\", \"utm_x\", \"utm_y\"]] = traffic_points.apply(get_lat_lon_utm, axis=1)\n",
    "\n",
    "traffic_points = traffic_points.rename(columns = {'nombre':'name'})\n",
    "\n",
    "traffic_points.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2de385",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"shared_data/districts/districts.geojson\", \"r\") as geojson:\n",
    "    geodata = json.load(geojson)\n",
    "\n",
    "df_districts = pd.DataFrame([], columns=[\"name\", \"latitude\",\n",
    "                                         \"longitude\", \"utm_x\",\n",
    "                                         \"utm_y\"])\n",
    "for district in geodata[\"features\"]:\n",
    "    # Get district name\n",
    "    district_name = district[\"properties\"][\"NOMBRE\"]\n",
    "    \n",
    "    # Get district coordinates\n",
    "    district_coord = district[\"geometry\"][\"coordinates\"][0]\n",
    "    df_district = pd.DataFrame(district[\"geometry\"][\"coordinates\"][0], columns=[\"st_x\", \"st_y\"])\n",
    "    df_district[\"name\"] = district_name\n",
    "    \n",
    "    # Calculate correct utm\n",
    "    df_district[[\"latitude\", \"longitude\", \"utm_x\", \"utm_y\"]] = df_district.apply(get_lat_lon_utm, axis=1)\n",
    "    df_district = df_district.drop(columns=[\"st_x\", \"st_y\"])\n",
    "    \n",
    "    # Append to all districts dataframe\n",
    "    df_districts = pd.concat([df_districts, df_district]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "district_name = df_districts[\"name\"].unique()\n",
    "df_districts"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7ddedbcd29fde6a544c51d6b472c0f25abc03cef05b4c1054f5ff10445887693"
  },
  "kernelspec": {
   "display_name": "Python 3.7.0 ('SDAV')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
